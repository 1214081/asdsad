{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48823f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.4.3.20220106)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from snscrape) (3.3.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from snscrape) (4.10.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from snscrape) (4.6.3)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from snscrape) (2.28.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from beautifulsoup4->snscrape) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.26.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: Sastrawi in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: pyLDAvis in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyLDAvis) (58.0.4)\n",
      "Requirement already satisfied: gensim in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.3.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.0.post1)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.20.3)\n",
      "Requirement already satisfied: numexpr in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: funcy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.17)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.7.1)\n",
      "Requirement already satisfied: future in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.24.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (2.0.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (6.3.0)\n",
      "Requirement already satisfied: Cython==0.29.32 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (0.29.32)\n",
      "Requirement already satisfied: pyfume in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim->pyLDAvis) (0.2.25)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: simpful in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (2.9.0)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (1.8.1)\n",
      "Requirement already satisfied: miniful in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (0.0.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (2.28.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (2021.10.8)\n",
      "Requirement already satisfied: gensim in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: Cython==0.29.32 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (0.29.32)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: pyfume in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.3.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: simpful in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.9.0)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: miniful in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.28.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!Pip install snscrape\n",
    "!pip install emoji\n",
    "!pip install Sastrawi\n",
    "!pip install pyLDAvis\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e1a4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as snt \n",
    "import pandas as pd \n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "import ast \n",
    "\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd06685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as snt \n",
    "import pandas as pd \n",
    "query = \"resesi since:2022-10-03 until:2022-12-31 lang:id\"\n",
    "tweets = [] \n",
    "limit = 1000000\n",
    "for tweet in snt.TwitterSearchScraper(query).get_items(): \n",
    "    # print(vars(tweet)) \n",
    "    # break \n",
    "    if len(tweets) == limit: \n",
    "        break \n",
    "    else: tweets.append([tweet.date, tweet.content]) \n",
    "    tweet_data = pd.DataFrame(tweets, columns=[\"Date\", \"Tweets\"]) \n",
    "#     print(tweet_data.head()) \n",
    "#     tweet_data.to_csv('resesi1b2.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c74c852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d70cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data.to_csv('resesi_ekonomi.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53b60c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Banyak negara diperkirakan mengalami resesi. M...</td>\n",
       "      <td>2022-12-30 21:15:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Banyak negara diperkirakan mengalami resesi. M...</td>\n",
       "      <td>2022-12-30 21:12:22+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Pemerintah harus memfokuskan diri untuk menjag...</td>\n",
       "      <td>2022-12-30 20:52:11+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>• perhatiin savings, beli yg lumayan gede nya ...</td>\n",
       "      <td>2022-12-30 20:30:53+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@tanyarlfes Ga terjadi resesi</td>\n",
       "      <td>2022-12-30 19:38:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>resesi global 2023. #ForzaMilan https://t.co/B...</td>\n",
       "      <td>2022-12-30 19:22:58+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Puan Maharani menyebut, pengusaha muda yang te...</td>\n",
       "      <td>2022-12-30 18:52:08+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Makin rajin lg nabung kalau bisa 40% gaji gw d...</td>\n",
       "      <td>2022-12-30 18:26:39+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023 gak jadi resesi soalnya jeben berlayar ma...</td>\n",
       "      <td>2022-12-30 18:21:52+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@komedigelaap Mungkin stress akibat resesi eko...</td>\n",
       "      <td>2022-12-30 17:48:18+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“Pertama, pemerintah perlu mempercepat antisip...</td>\n",
       "      <td>2022-12-30 17:42:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Tinggal menghitung hari ini “Resesi”</td>\n",
       "      <td>2022-12-30 17:21:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Karena 2023 diprediksi resesi, katanya Bulan D...</td>\n",
       "      <td>2022-12-30 17:17:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Tadi sore ke klinik gigi dan belajar istilah m...</td>\n",
       "      <td>2022-12-30 17:11:52+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweets  \\\n",
       "9   Banyak negara diperkirakan mengalami resesi. M...   \n",
       "10  Banyak negara diperkirakan mengalami resesi. M...   \n",
       "11  Pemerintah harus memfokuskan diri untuk menjag...   \n",
       "12  • perhatiin savings, beli yg lumayan gede nya ...   \n",
       "13                      @tanyarlfes Ga terjadi resesi   \n",
       "14  resesi global 2023. #ForzaMilan https://t.co/B...   \n",
       "15  Puan Maharani menyebut, pengusaha muda yang te...   \n",
       "16  Makin rajin lg nabung kalau bisa 40% gaji gw d...   \n",
       "17  2023 gak jadi resesi soalnya jeben berlayar ma...   \n",
       "18  @komedigelaap Mungkin stress akibat resesi eko...   \n",
       "19  “Pertama, pemerintah perlu mempercepat antisip...   \n",
       "20               Tinggal menghitung hari ini “Resesi”   \n",
       "21  Karena 2023 diprediksi resesi, katanya Bulan D...   \n",
       "22  Tadi sore ke klinik gigi dan belajar istilah m...   \n",
       "\n",
       "                        Date  \n",
       "9  2022-12-30 21:15:26+00:00  \n",
       "10 2022-12-30 21:12:22+00:00  \n",
       "11 2022-12-30 20:52:11+00:00  \n",
       "12 2022-12-30 20:30:53+00:00  \n",
       "13 2022-12-30 19:38:10+00:00  \n",
       "14 2022-12-30 19:22:58+00:00  \n",
       "15 2022-12-30 18:52:08+00:00  \n",
       "16 2022-12-30 18:26:39+00:00  \n",
       "17 2022-12-30 18:21:52+00:00  \n",
       "18 2022-12-30 17:48:18+00:00  \n",
       "19 2022-12-30 17:42:10+00:00  \n",
       "20 2022-12-30 17:21:31+00:00  \n",
       "21 2022-12-30 17:17:00+00:00  \n",
       "22 2022-12-30 17:11:52+00:00  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfc = pd.read_csv(\"resesi_ekonomi.csv\");\n",
    "# dfc = tweet_data\n",
    "dfc.loc[9:22,[\"Tweets\",\"Date\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9eaf60bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Puan Maharani menyebut, pengusaha muda yang terhimpun dalam HIPMI harus siap menghadapi ancaman resesi global yang diperkirakan melanda seluruh negara di dunia tahun depan.\\n\\nPuan Capres PDIP\\nhttps://t.co/VOApR3g6VP'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc['Tweets'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "816bfbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc=[\"Tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddefebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50426 entries, 0 to 50425\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype              \n",
      "---  ------  --------------  -----              \n",
      " 0   Date    50426 non-null  datetime64[ns, UTC]\n",
      " 1   Tweets  50426 non-null  object             \n",
      "dtypes: datetime64[ns, UTC](1), object(1)\n",
      "memory usage: 788.0+ KB\n"
     ]
    }
   ],
   "source": [
    "dfc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "954593b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfc = dfc.drop('Number of likes', axis=1)\n",
    "# dfc = dfc.drop('User', axis=1)\n",
    "# dfc = dfc.drop('Tweeted from', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc6613",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dfc = dfc.drop('clear', axis=1)\n",
    "except:\n",
    "    print(\"\")\n",
    "clean_tweets = []\n",
    "token_tweets = []\n",
    "freq_words = []\n",
    "\n",
    "# stopword\n",
    "stop_factory = StopWordRemoverFactory().get_stop_words() #load defaul stopword\n",
    "more_stopword = ['•',\"stengah\", \"kopit\", \"china\",\"g\",\"yaaaaa\",\"gengs\",\"gada\",\"dgn\",\"nich\",\"yg\",\"padan\", \"juoro\",\"nya\",\n",
    "\"js\",\"kl\",\"\",\"co\",\"ga\",\"lg\",\"gw\",\"jg\",\"walu\",\"grrabbpoodd\",\"klo\",\"jeben\",\"makane\",\"kakean\",\"sek\",\"mb\",\"skp\",\"tpi\",\"bgt\"\n",
    ",\"lgi\",\"lu\",\"rb\",\"rban\",\"mura\",\"pd\",\"nih\",\"lii\",\"enel\",\"dr\",\"exo\",\"ipo\",\"trus\",\"d\",\"shm\",\"skrg\",\"byk\",\"mang\",\"ots\",\"dah\"\n",
    "\"dg\",\"bp\",\"n\",\"arsjadrasjid\"] #menambahkan stopword\n",
    "data_stopword = stop_factory + more_stopword #menggabungkan stopword\n",
    "\n",
    "for tweet in tqdm(dfc['Tweets']):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "#     tweet = \" \".join(tweet.split())\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    tweet = re.sub(r\"\\d+\", \"\", tweet) # Remove number\n",
    "    tweet = ''.join(c for c in tweet if c not in emoji.EMOJI_DATA) #Remove Emojis\n",
    "    tweet = tweet.replace('\"','') #remove quotation mark\n",
    "    tweet = tweet.lower() #Lower Case\n",
    "    tweet = tweet.strip() # Remove Whitespace\n",
    "#     tweet = re.sub(r'[^\\w\\s]', '', (tweet)) #Remove Punctuation\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation)) #Remove Punctuation \n",
    "    \n",
    "    \n",
    "    #Stopword\n",
    "    dictionary = ArrayDictionary(data_stopword)\n",
    "    swr = StopWordRemover(dictionary)\n",
    "    tweet = swr.remove(tweet)\n",
    "    \n",
    "    #Tokenization\n",
    "    tokens = nltk.tokenize.word_tokenize(tweet)\n",
    "    tweet = \" \".join(tokens) #Disatukan Kembali\n",
    "    \n",
    "    freq_words.append(nltk.FreqDist(word_tokenize(tweet)))\n",
    "    \n",
    "    token_tweets.append(tokens)\n",
    "    clean_tweets.append(tweet)\n",
    "dfc['clear'] = clean_tweets\n",
    "dfc['token_tweets'] = token_tweets\n",
    "dfc['freq_words'] = freq_words\n",
    "\n",
    "all_freq_words = nltk.FreqDist(sum(dfc['clear'].map(word_tokenize), []))\n",
    "\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d15d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Sastrawi package\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in dfc['token_tweets']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    print(term,\":\" ,term_dict[term])\n",
    "    \n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "dfc['stemmed_tweet'] = dfc['token_tweets'].swifter.apply(get_stemmed_term)\n",
    "print(dfc['stemmed_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c082c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b8c62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df\n",
    "# FreqWord = pd.DataFrame(all_freq_words.items(), columns=['word', 'frequency']).sort_values(by='frequency', ascending=False)\n",
    "# for ind in FreqWord.index:\n",
    "#     print(FreqWord['word'][ind], FreqWord['frequency'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff44ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['clear'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec21da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = dfc.drop(dfc[dfc.clear == ''].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc[\"clear\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98717937",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.drop_duplicates(subset='clear',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef42ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dea90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine sentiment polarity of tweets using indonesia sentiment lexicon (source : https://github.com/fajri91/InSet)\n",
    "# Loads lexicon positive and negative data\n",
    "lexicon_positive = dict()\n",
    "import csv\n",
    "with open('lexicon_dictionary/positive.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        lexicon_positive[row[0]] = int(row[1])\n",
    "\n",
    "lexicon_negative = dict()\n",
    "import csv\n",
    "with open('lexicon_dictionary/negative.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        lexicon_negative[row[0]] = int(row[1])\n",
    "        \n",
    "# Function to determine sentiment polarity of tweets        \n",
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    #for word in text:\n",
    "    score = 0\n",
    "    for word in text:\n",
    "        if (word in lexicon_positive):\n",
    "            score = score + lexicon_positive[word]\n",
    "    for word in text:\n",
    "        if (word in lexicon_negative):\n",
    "            score = score + lexicon_negative[word]\n",
    "    polarity=''\n",
    "    s=0\n",
    "    if (score > 0):\n",
    "        polarity = 'positive'\n",
    "        s=1\n",
    "    elif (score < 0):\n",
    "        polarity = 'negative'\n",
    "        s=-1\n",
    "    else:\n",
    "        polarity = 'neutral'\n",
    "        s=0\n",
    "    return score, polarity, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d311a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dfc['token_tweets'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "results = list(zip(*results))\n",
    "dfc['polarity_score'] = results[0]\n",
    "dfc['polarity'] = results[1]\n",
    "dfc['score'] = results[2]\n",
    "print(dfc['polarity'].value_counts())\n",
    "\n",
    "# Export to csv file\n",
    "# tweets.to_csv(r'25k_tweets_data_clean_polarity.csv', index = False, header = True,index_label=None)\n",
    "\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf6cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f96ab19b",
   "metadata": {},
   "source": [
    "Visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21336274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef062e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "sizes = [count for count in dfc['polarity'].value_counts()]\n",
    "labels = list(dfc['polarity'].value_counts().index)\n",
    "explode = (0.1, 0, 0)\n",
    "ax.pie(x = sizes, labels = labels, autopct = '%1.1f%%', explode = explode, textprops={'fontsize': 14})\n",
    "ax.set_title('Sentiment Polarity on Tweets Data \\n (total = 23699 tweets)', fontsize = 16, pad = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 3000)\n",
    "positive_tweets = dfc[dfc['polarity'] == 'positive']\n",
    "positive_tweets = positive_tweets[['clear', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=False).reset_index(drop = True)\n",
    "positive_tweets.index += 1\n",
    "positive_tweets[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df06b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 3000)\n",
    "negative_tweets = dfc[dfc['polarity'] == 'negative']\n",
    "negative_tweets = negative_tweets[['clear', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=True)[0:10].reset_index(drop = True)\n",
    "negative_tweets.index += 1\n",
    "negative_tweets[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c42927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word cloud\n",
    "\n",
    "list_words=''\n",
    "for tweet in dfc['token_tweets']:\n",
    "    for word in tweet:\n",
    "        list_words += ' '+(word)\n",
    "\n",
    "wordcloud = WordCloud(width = 600, height = 400, background_color = 'black', min_font_size = 10).generate(list_words)\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "ax.set_title('Word Cloud of Tweets Data', fontsize = 18)\n",
    "ax.grid(False)\n",
    "ax.imshow((wordcloud))\n",
    "fig.tight_layout(pad=0)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e59791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nltk.FreqDist(sum(dfc['clear'].map(word_tokenize), []))\n",
    "# list_words\n",
    "tokens = nltk.tokenize.word_tokenize(list_words)\n",
    "fdist=nltk.FreqDist(tokens)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_with_sentiment(text):\n",
    "    positive_words=[]\n",
    "    negative_words=[]\n",
    "    for word in text:\n",
    "        score_pos = 0\n",
    "        score_neg = 0\n",
    "        if (word in lexicon_positive):\n",
    "            score_pos = lexicon_positive[word]\n",
    "        if (word in lexicon_negative):\n",
    "            score_neg = lexicon_negative[word]\n",
    "        \n",
    "        if (score_pos + score_neg > 0):\n",
    "            positive_words.append(word)\n",
    "        elif (score_pos + score_neg < 0):\n",
    "            negative_words.append(word)\n",
    "            \n",
    "    return positive_words, negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_words = dfc['token_tweets'].apply(words_with_sentiment)\n",
    "sentiment_words = list(zip(*sentiment_words))\n",
    "positive_words = sentiment_words[0]\n",
    "negative_words = sentiment_words[1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2,figsize = (12, 10))\n",
    "list_words_postive=''\n",
    "for row_word in positive_words:\n",
    "    for word in row_word:\n",
    "        list_words_postive += ' '+(word)\n",
    "wordcloud_positive = WordCloud(width = 800, height = 600, background_color = 'black', colormap = 'Greens'\n",
    "                               , min_font_size = 10).generate(list_words_postive)\n",
    "ax[0].set_title('Word Cloud of Positive Words on Tweets Data \\n (based on Indonesia Sentiment Lexicon)', fontsize = 14)\n",
    "ax[0].grid(False)\n",
    "ax[0].imshow((wordcloud_positive))\n",
    "fig.tight_layout(pad=0)\n",
    "ax[0].axis('off')\n",
    "\n",
    "list_words_negative=''\n",
    "for row_word in negative_words:\n",
    "    for word in row_word:\n",
    "        list_words_negative += ' '+(word)\n",
    "wordcloud_negative = WordCloud(width = 800, height = 600, background_color = 'black', colormap = 'Reds'\n",
    "                               , min_font_size = 10).generate(list_words_negative)\n",
    "ax[1].set_title('Word Cloud of Negative Words on Tweets Data \\n (based on Indonesia Sentiment Lexicon)', fontsize = 14)\n",
    "ax[1].grid(False)\n",
    "ax[1].imshow((wordcloud_negative))\n",
    "fig.tight_layout(pad=0)\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb68a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['Date'] = pd.to_datetime(dfc['Date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb30125",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce9059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753eb0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_day = pd.to_datetime(dfc['Date']).dt.to_period('D').value_counts().sort_index()\n",
    "by_day.index = pd.PeriodIndex(by_day.index)\n",
    "df_day = by_month.rename_axis('day').reset_index(name='counts')\n",
    "df_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71127f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot results ##\n",
    "f = plt.figure()\n",
    "f.set_figwidth(12)\n",
    "f.set_figheight(4)\n",
    "plt.plot(df_day.index,df_day.counts)\n",
    "plt.xticks(df_day.index,df_day.day)\n",
    "plt.xticks(rotation = 60)\n",
    "# plt.legend()\n",
    "plt.ylabel('# of tweets')\n",
    "plt.xlabel(\"time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_freq = pd.DataFrame(list(all_freq_words.items()), columns = [\"Word\",\"Frequency\"]).sort_values(by='Frequency', ascending=False)\n",
    "data_freq = df_freq.head()\n",
    "data_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f27ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08da39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq = pd.DataFrame(list(all_freq_words.items()), columns = [\"Word\",\"Frequency\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = data_freq['Word']\n",
    "price = data_freq['Frequency']\n",
    " \n",
    "# Figure Size\n",
    "fig = plt.figure(figsize =(10, 7))\n",
    " \n",
    "# Horizontal Bar Plot\n",
    "plt.bar(name[0:10], price[0:10])\n",
    " \n",
    "# Show Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d03ae",
   "metadata": {},
   "source": [
    "NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfc['clear']\n",
    "Y = dfc['score']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ee5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF vectorization of text\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "tfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train) \n",
    "tfidf_test_vectors = tfidf_vectorizer.transform(X_test)\n",
    "print(\"n_samples: %d, n_features: %d\" % tfidf_train_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training and prediction\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(tfidf_train_vectors, Y_train)\n",
    "\n",
    "y_pred = naive_bayes_classifier.predict(tfidf_test_vectors)\n",
    "print(classification_report(Y_test, y_pred)) #model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy = ', round(accuracy_score(Y_test, y_pred), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe93c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_mat = confusion_matrix(Y_test, y_pred)\n",
    "cnf_mat #confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cnf_mat, annot = True, fmt = '', cmap = 'Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = np.diag(cnf_mat)\n",
    "TP = TP.astype(float)\n",
    "FN = cnf_mat.sum(axis = 1) - TP\n",
    "FN = FN.astype(float)\n",
    "FP = cnf_mat.sum(axis = 0) - TP \n",
    "FP = FP.astype(float)\n",
    "TN = cnf_mat.sum() - (TP + FN + FP)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity\n",
    "TPR = TP/(TP+FN)\n",
    "\n",
    "# Specificity\n",
    "TNR = TN/(TN+FP) \n",
    "\n",
    "#Precision\n",
    "FPR = FP/(TN+FP)\n",
    "\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "# Classification error\n",
    "Err = (FP+FN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('Recall = ', TPR.round(2))\n",
    "print('Precision = ', FPR.round(2))\n",
    "print('Accuracy = ', ACC.round(2))\n",
    "print('Classification error = ', Err.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edc4a5",
   "metadata": {},
   "source": [
    "LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cfd112",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "texts = []\n",
    "\n",
    "for tweet in dfc['token_tweets']:\n",
    "    texts.append(tweet)\n",
    "    documents.append(tweet)\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a78e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "document_term = [dictionary.doc2bow(document) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fcd125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e38d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(document_term, num_topics=10, id2word = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291bf991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
